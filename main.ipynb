{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import pickle\n",
    "import datetime\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torchvision import models, transforms\n",
    "from transformers import pipeline\n",
    "import easyocr\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ изображений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Наличие текста\n",
    "def detect_text(image_path):\n",
    "    reader = easyocr.Reader(['en'])\n",
    "    result = reader.readtext(image_path)\n",
    "    return 1 if result else 0\n",
    "\n",
    "# Оценка красочности\n",
    "def calculate_colorfulness(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    (B, G, R) = cv2.split(image.astype(\"float\"))\n",
    "    rg = np.abs(R - G)\n",
    "    yb = np.abs(0.5 * (R + G) - B)\n",
    "    rg_mean, rg_std = np.mean(rg), np.std(rg)\n",
    "    yb_mean, yb_std = np.mean(yb), np.std(yb)\n",
    "    std_root = np.sqrt((rg_std ** 2) + (yb_std ** 2))\n",
    "    mean_root = np.sqrt((rg_mean ** 2) + (yb_mean ** 2))\n",
    "    colorfulness = std_root + (0.3 * mean_root)\n",
    "    return min(10, max(1, int(colorfulness / 10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запрос к llama_request\n",
    "def llama_request(prompt):\n",
    "    \"\"\"Отправка запроса к LLM и получение ответа.\"\"\"\n",
    "    # URL вашего сервера\n",
    "    url = 'http://localhost:11434/api/chat'\n",
    "    payload = {\n",
    "        \"model\": \"llama3.1:8b\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"stream\": False,\n",
    "        \"max_token\": 10000\n",
    "    }\n",
    "\n",
    "    # Отправка POST-запроса\n",
    "    response = requests.post(\n",
    "        url,\n",
    "        data=json.dumps(payload)\n",
    "    )\n",
    "    return response.json()['message']['content']\n",
    "\n",
    "# Анализ текста с помощью llama\n",
    "def analyze_post(post_text):\n",
    "    # 1. Тематика поста\n",
    "    topic_prompt = f\"Определи основную тему текста: \\\"{post_text}\\\". Ответь только одним словом без точки\"\n",
    "    topics = llama_request(topic_prompt)\n",
    "    \n",
    "    # 2. Структура текста\n",
    "    word_count = len(post_text.split())\n",
    "    char_count = len(post_text)\n",
    "    \n",
    "    # Проверка наличия заголовков и списков\n",
    "    headers = len(re.findall(r'(^|\\n)(#+\\s|\\s*\\d+\\.\\s|\\*\\s)', post_text))\n",
    "    paragraphs = post_text.strip().split(\"\\n\\n\")\n",
    "    has_lists = bool(re.search(r'(^|\\n)(\\d+\\.\\s|\\*\\s)', post_text))\n",
    "    \n",
    "    # 3. Язык и стиль\n",
    "    tone_prompt = f\"Определи тональность текста одним словом (позитивная, негативная, нейтральная). Ответ только слово - тональность.: \\\"{post_text}\\\".\"\n",
    "    tone = llama_request(tone_prompt)\n",
    "    \n",
    "    slang_prompt = f\"Оцени простоту текста ответив только оценку 1 до 10 без точки в конце. Оцени насколько сложен текст с точки зрения трудных слов, сложных/длинных граматических констркуций и тому подобное. \\\"{post_text}\\\".\"\n",
    "    uses_slang = llama_request(slang_prompt)\n",
    "    \n",
    "    emotion_prompt = f\"Определи эмоциональную окраску текста одним словом: \\\"{post_text}\\\". Пример ответа: радостная, грустная, нейтральная. Ответь только слово - эмоциональная окраска. Больше ничего не пиши.\"\n",
    "    emotion = llama_request(emotion_prompt)\n",
    "    \n",
    "    # Результаты анализа\n",
    "    result = {\n",
    "        \"Тематика\": topics,\n",
    "        \"Структура текста\": {\n",
    "            \"Количество слов\": word_count,\n",
    "            \"Количество символов\": char_count,\n",
    "            \"Количество заголовков\": headers,\n",
    "            \"Количество абзацев\": len(paragraphs),\n",
    "            \"Есть списки\": has_lists,\n",
    "        },\n",
    "        \"Язык и стиль\": {\n",
    "            \"Тональность\": tone,\n",
    "            \"Сложность текста\": uses_slang,\n",
    "            \"Эмоциональная окраска\": emotion,\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сбор данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = '7467ea587467ea587467ea58a97776aa9e774677467ea58173abbe1a90fd6c976a91670'\n",
    "owner_id = '-23242408'\n",
    "# owner_id2 = 68661738\n",
    "count = 0\n",
    "offset = 0\n",
    "\n",
    "img = \"/Users/aleksandrcuvpilo/Desktop/Programming/vscode/python_folder/project/vkarminey/downloaded_image.jpg\"\n",
    "\n",
    "\n",
    "f = open(\"file.pkl\", \"wb\")\n",
    "with open(\"yes.txt\", \"r\") as file:\n",
    "    files = file.read()\n",
    "    emoji = files.split()\n",
    "temp = open('temp.txt', \"wb\")\n",
    "def getjson(url, data=None):\n",
    "    response = requests.get(url, params=data)\n",
    "    response = response.json()\n",
    "    return response\n",
    "\n",
    "def get_all_postss(access_token, owner_id, count=500, offset=0):\n",
    "    all_posts = []\n",
    "    k = 0\n",
    "    last_post_date = -1\n",
    "\n",
    "    while True:\n",
    "        time.sleep(0.9)\n",
    "        wall = getjson('https://api.vk.com/method/wall.get',\n",
    "                        {'owner_id': owner_id, 'offset': offset, 'count': count, 'access_token': access_token, 'v': '5.131'})\n",
    "        count_posts = wall['response']['count']\n",
    "        posts = wall['response']['items']\n",
    "\n",
    "        all_posts.extend(posts)\n",
    "\n",
    "        last_post_date = int(datetime.datetime.fromtimestamp(int(all_posts[-1]['date'])).strftime('%Y'))\n",
    "\n",
    "        # if k % 50 == 0:\n",
    "        #     print(k)\n",
    "        # k += 1\n",
    "        if last_post_date == 2023:\n",
    "            break\n",
    "        else:\n",
    "            offset += 100\n",
    "    return count_posts\n",
    "\n",
    "def get_all_posts(access_token, owner_id, count=500, offset=0):\n",
    "    all_posts = []\n",
    "    k = 0\n",
    "    last_post_date = -1\n",
    "\n",
    "    while True:\n",
    "        time.sleep(0.9)\n",
    "        wall = getjson('https://api.vk.com/method/wall.get',\n",
    "                        {'owner_id': owner_id, 'offset': offset, 'count': count, 'access_token': access_token, 'v': '5.131'})\n",
    "        count_posts = wall['response']['count']\n",
    "        posts = wall['response']['items']\n",
    "\n",
    "        all_posts.extend(posts)\n",
    "\n",
    "        last_post_date = int(datetime.datetime.fromtimestamp(int(all_posts[-1]['date'])).strftime('%Y'))\n",
    "\n",
    "        # if k % 50 == 0:\n",
    "        #     print(k)\n",
    "        # k += 1\n",
    "        if last_post_date == 2023:\n",
    "            break\n",
    "        else:\n",
    "            offset += 100\n",
    "    return all_posts\n",
    "\n",
    "def filter_data(all_posts):\n",
    "    filtered_data = []\n",
    "    photo_cnt = 0\n",
    "    for post in all_posts:\n",
    "        try:\n",
    "            id = post['id']\n",
    "        except:\n",
    "            id = ' '\n",
    "        try:\n",
    "            text = post['text']\n",
    "        except:\n",
    "            text = 'текста нет'\n",
    "        try:\n",
    "            likes = post['likes']['count']\n",
    "        except:\n",
    "            likes = 0\n",
    "        try:\n",
    "            date = post['date']\n",
    "        except:\n",
    "            date = 0\n",
    "        try:\n",
    "            cnt_comm = len(post['comments'])\n",
    "        except:\n",
    "            cnt_comm = 0\n",
    "        photo_ids = []\n",
    "        for i in post['attachments']:\n",
    "            if i['type'] == 'photo':\n",
    "                #print(i['photo'])\n",
    "                #width = i['photo']['width']\n",
    "                #height = i['photo']['height']\n",
    "                #print(i['photo']['sizes'])\n",
    "                for j in i['photo']['sizes']:\n",
    "                    if j['type'] == 'x':\n",
    "                        photo_cnt = 1\n",
    "                        photo_ids.append(j['url'])\n",
    "                        break\n",
    "                if(photo_cnt == 0):\n",
    "                    photo_ids.append(i['photo']['sizes'][0]['url'])\n",
    "\n",
    "        filtered_post = {'id': id, 'text': text, 'likes': likes, 'date': date, 'cnt_comm': cnt_comm, 'photo_ids': photo_ids}\n",
    "\n",
    "        filtered_data.append(filtered_post)\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "def get_all_liked_lists(access_token, owner_id, liked_object_id, count=1000, offset=0, friends_only=0):\n",
    "    time.sleep(0.5)\n",
    "    api_query = getjson('https://api.vk.com/method/likes.getList',\n",
    "                        {'access_token': access_token, 'type': 'post', 'owner_id': owner_id, 'item_id': liked_object_id,\n",
    "                            'filter': 'likes', 'friends_only': friends_only, 'count': count, 'v': '5.131'})\n",
    "    Users_count = api_query['response']['count']\n",
    "    List_of_users = api_query['response']['items']\n",
    "    return Users_count, List_of_users\n",
    "\n",
    "def get_all_users_bdate(access_token, user_ids, fields, count=1000, offset=0, friends_only=0):\n",
    "    time.sleep(0.5)\n",
    "    api_query_user_info = getjson('https://api.vk.com/method/users.get',\n",
    "                                    {'access_token': access_token, 'user_ids': user_id, 'fields': 'bdate, sex', 'count': count,\n",
    "                                    'v': '5.131'})\n",
    "    User_Birth_date = api_query_user_info['response']\n",
    "    # User_Birth_date = api_query_user_info['response'][0]\n",
    "    return User_Birth_date\n",
    "\n",
    "def get_subs(access_token, owner_id, count=100, offset=0):\n",
    "    listo = []\n",
    "    time.sleep(1)\n",
    "    subs = getjson('https://api.vk.com/method/groups.getMembers',\n",
    "                    {'group_id': owner_id, 'sort': 'time_asc', 'offset': offset, 'count': count, 'fields': 'bdate', 'access_token': access_token, 'v': '5.131'})\n",
    "    csus = subs['response']['count']\n",
    "    for i in range(csus):\n",
    "        time.sleep(1)\n",
    "        subs = getjson('https://api.vk.com/method/groups.getMembers',\n",
    "                        {'group_id': owner_id, 'sort': 'time_asc', 'offset': offset, 'count': count, 'fields': 'bdate',\n",
    "                        'access_token': access_token, 'v': '5.131'})\n",
    "        csubs = subs['response']['items']\n",
    "        listo.extend(csubs)\n",
    "    return listo\n",
    "\n",
    "def anylyse_url(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        with open(img, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "        #print(f\"Image downloaded successfully and saved as '{img}'.\")\n",
    "    else:\n",
    "        pass\n",
    "        #print(f\"Failed to download image. HTTP status code: {response.status_code}\")\n",
    "    image_path = img\n",
    "    has_text = detect_text(image_path)\n",
    "    #has_people = detect_people(image_path)\n",
    "    #objects = detect_objects(image_path)\n",
    "    colorfulness = calculate_colorfulness(image_path)\n",
    "    os.remove(img)\n",
    "    return has_text, colorfulness\n",
    "\n",
    "count = 0\n",
    "offset = 0\n",
    "main_posts = get_all_postss(access_token, owner_id)\n",
    "if main_posts > 1000:\n",
    "    main_posts = 999\n",
    "\n",
    "all_posts = get_all_posts(access_token, owner_id)\n",
    "final_filter = filter_data(all_posts)\n",
    "ID_list = []\n",
    "txt_list = []\n",
    "likes_list = []\n",
    "date_list = []\n",
    "cnt_comm_list = []\n",
    "photo_ids_list = []\n",
    "for post1 in final_filter:\n",
    "    ID_list.append(post1['id'])\n",
    "    txt_list.append(post1['text'])\n",
    "    likes_list.append(post1['likes'])\n",
    "    date_list.append(post1['date'])\n",
    "    cnt_comm_list.append(post1['cnt_comm'])\n",
    "    photo_ids_list.append(post1['photo_ids'])\n",
    "\n",
    "User_lists_collection = []\n",
    "Final_list = {}\n",
    "t = 0\n",
    "for item in ID_list:\n",
    "    text_id = txt_list[t]\n",
    "    tsplit = text_id.split()\n",
    "    emojicount = 0\n",
    "    tagscount = 0\n",
    "    wordcount = 0\n",
    "    linkcount = 0\n",
    "    for i in range(len(tsplit)):\n",
    "        tsp = tsplit[i]\n",
    "        x = tsp.find('#')\n",
    "        if x != (-1):\n",
    "            tagscount += 1\n",
    "            wordcount -= 1\n",
    "        for j in range(len(tsp)):\n",
    "            if tsp[j] in emoji:\n",
    "                emojicount +=1\n",
    "        if tsp not in emoji:\n",
    "            wordcount += 1\n",
    "        y = tsp.find('.')\n",
    "        if y != (-1) and y != len(tsp)-1:\n",
    "            linkcount += 1\n",
    "            wordcount -= 1\n",
    "    liked_object_id = item\n",
    "    User_list_of_responses = get_all_liked_lists(access_token, owner_id, liked_object_id)\n",
    "    text_analyze = analyze_post(text_id)\n",
    "    all_text = 0\n",
    "    all_people = 0\n",
    "    all_objects = []\n",
    "    all_colorfulness = 0\n",
    "    cnt_text = 0\n",
    "    cnt_people = 0\n",
    "    cnt_colorfulness = 0\n",
    "    for j in photo_ids_list[t]:\n",
    "        has_text, colorfulness = anylyse_url(j)\n",
    "        all_text += int(has_text)\n",
    "        #all_people += int(has_people)\n",
    "        #all_objects.extend(objects)\n",
    "        all_colorfulness += int(colorfulness)\n",
    "        cnt_text += 1\n",
    "        cnt_colorfulness += 1\n",
    "    try:\n",
    "        medium_photo_text = all_text/cnt_text\n",
    "    except:\n",
    "        medium_photo_text = np.nan\n",
    "    try:\n",
    "        medium_photo_colorfulness = all_colorfulness/cnt_colorfulness\n",
    "    except:\n",
    "        medium_photo_colorfulness = np.nan\n",
    "\n",
    "    list_of_users_ids = User_list_of_responses[1]\n",
    "    user_id = str(list_of_users_ids).strip('[]')\n",
    "    person_bdate = get_all_users_bdate(access_token, owner_id, user_id)\n",
    "\n",
    "    nfl = [i for i in person_bdate]\n",
    "    female_all = [i for i in nfl if 'sex' in i and i['sex'] == 1]\n",
    "    male_all = [i for i in nfl if 'sex' in i and i['sex'] == 2]\n",
    "\n",
    "    new_filtered_list = [i for i in person_bdate if 'bdate' in i and len(i['bdate']) > 6 and int(i['bdate'][-4:]) >= 2002 and int(i['bdate'][-4:]) <= 2010]\n",
    "    female_school = [i for i in new_filtered_list if 'sex' in i and i['sex'] == 1]\n",
    "    male_school = [i for i in new_filtered_list if 'sex' in i and i['sex'] == 2]\n",
    "\n",
    "    nfl2 = [i for i in person_bdate if 'bdate' in i and len(i['bdate']) > 6 and int(i['bdate'][-4:]) <= 1993]\n",
    "    female_eldery = [i for i in nfl2 if 'sex' in i and i['sex'] == 1]\n",
    "    male_eldery = [i for i in nfl2 if 'sex' in i and i['sex'] == 2]\n",
    "\n",
    "    f = 0\n",
    "    list_of_likes = len(new_filtered_list)\n",
    "    list_of_likes2 = len(nfl2)\n",
    "    lol = len(nfl)\n",
    "    if list_of_likes != 0:\n",
    "        f = (list_of_likes / lol) * 100\n",
    "    f = str(f) + '%'\n",
    "    \n",
    "    Final_list = {'items': liked_object_id, 'text_id': text_id, 'emoji': emojicount, 'tags': tagscount, 'links': linkcount, 'words': wordcount, 'count': User_list_of_responses[0],\n",
    "                    'list of users': User_list_of_responses[1], 'likes_all': lol, 'likes_count (<21)': list_of_likes, 'likes_count (>31)': list_of_likes2, 'date': date_list[t], 'cnt_comm': cnt_comm_list[t], 'photo_ids': photo_ids_list[t], 'theme': text_analyze['Тематика'],\n",
    "                    'headers_cnt': text_analyze['Структура текста']['Количество заголовков'], 'parts_cnt': text_analyze['Структура текста']['Количество абзацев'], 'tone': text_analyze['Язык и стиль']['Тональность'],\n",
    "                    'difficult': text_analyze['Язык и стиль']['Сложность текста'], 'emote': text_analyze['Язык и стиль']['Эмоциональная окраска'], 'medium_photo_text': medium_photo_text, 'medium_photo_colorfulness': medium_photo_colorfulness}\n",
    "    User_lists_collection.append(Final_list)\n",
    "    t += 1\n",
    "    df = pd.DataFrame(User_lists_collection)\n",
    "    df.to_csv(f'file{t}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_to_merge = ['vtb.csv', 'tink.csv', 'sber.csv', 'file250.csv']\n",
    "deli = 0\n",
    "for i in files_to_merge:\n",
    "    df1 = pd.read_csv(i)\n",
    "    for j in range(len(df1['likes_all'])):\n",
    "        if i == 'vtb.csv':\n",
    "            deli = 0.65\n",
    "        elif i == 'tink.csv':\n",
    "            deli = 1.3\n",
    "        elif i == 'sber.csv':\n",
    "            deli = 3.37\n",
    "        elif i == 'file250.csv':\n",
    "            deli = 0.43 \n",
    "        df.loc[df['items'] == df1['items'][j], 'likes'] = df1['likes_all'][j] / deli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_to_nearest_hour(time_str):\n",
    "    hours, minutes, _ = map(int, time_str.split(':'))\n",
    "    # Увеличиваем час, если минуты >= 30\n",
    "    if minutes >= 30:\n",
    "        hours += 1\n",
    "    return hours\n",
    "\n",
    "# Применение функции к колонке\n",
    "df['date'] = pd.to_datetime(df['date'], unit='s').dt.strftime('%H:%M:%S')\n",
    "df['hour'] = df['date'].apply(round_to_nearest_hour)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка эмоций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df['emote'])):\n",
    "    try:\n",
    "        promt = f'Это суждение об эмоции {df['emote'][i]}. Ты должен написать \"0\", если эмоция нейтральная, \"1\", если эмоция радостная/положительная, \"-1\", если эмоция грустная/отрицательная. Ответь только число'\n",
    "        ans = llama_request(promt)\n",
    "        df['emote'][i] = ans\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['emote'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('after_emote_result.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['emote'].unique())\n",
    "for i in range(len(df['emote'])):\n",
    "    if df['emote'][i] == '-1.':\n",
    "        df['emote'][i] = -1\n",
    "    elif df['emote'][i] == 'Нет':\n",
    "        df['emote'][i] = 0\n",
    "    elif df['emote'][i] == '0.':\n",
    "        df['emote'][i] = 0\n",
    "    elif df['emote'][i] == 'Я не могу ответить на этот вопрос в соответствии с правилами и политикой платформы. Я могу помочь вам с чем-то другим?':\n",
    "        df['emote'][i] = 0\n",
    "    elif df['emote'][i] == 'Используйте функцию анализа эмоций. Тон - 0 (нейтральный)':\n",
    "        df['emote'][i] = 0\n",
    "    elif df['emote'][i] == 'Радостная.':\n",
    "        df['emote'][i] = 1\n",
    "    elif df['emote'][i] == '1.':\n",
    "        df['emote'][i] = 1\n",
    "    elif df['emote'][i] == 'Ответ:  1.':\n",
    "        df['emote'][i] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выбросы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Боксплоты для выбросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Стиль графика 'dark_background'\n",
    "plt.style.use('default')\n",
    "\n",
    "# Функция для построения boxplot с кастомными настройками\n",
    "def plot_boxplots(df, columns, rows=2, cols=3, figsize=(18, 12)):\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, column in enumerate(columns):\n",
    "        # Создание boxplot для каждой колонки\n",
    "        sns.boxplot(data=df, x=column, ax=axes[i], color='#8A2BE2')\n",
    "\n",
    "        # Настройка графика\n",
    "        axes[i].set_title(f\"Boxplot для {column}\", fontsize=14, color='black')\n",
    "        axes[i].set_xlabel(column, fontsize=12, color='black')\n",
    "        axes[i].tick_params(axis='x', colors='black', labelsize=10)\n",
    "        axes[i].tick_params(axis='y', colors='black', labelsize=10)\n",
    "\n",
    "        # Убираем рамку вокруг графиков\n",
    "        '''\n",
    "        axes[i].spines['top'].set_visible(False)\n",
    "        axes[i].spines['right'].set_visible(False)\n",
    "        axes[i].spines['left'].set_visible(False)\n",
    "        axes[i].spines['bottom'].set_visible(False)\n",
    "        '''\n",
    "    \n",
    "    # Убираем пустые графики, если колонок меньше чем количество ячеек\n",
    "    if len(columns) < rows * cols:\n",
    "        for j in range(len(columns), rows * cols):\n",
    "            fig.delaxes(axes[j])\n",
    "\n",
    "    # Настройка интервалов между графиками\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Сохранение графика\n",
    "    plt.savefig('boxplots_grid.png', bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "\n",
    "# Пример использования\n",
    "columns_to_clean = ['words', 'likes_all']  # Здесь твои колонки\n",
    "plot_boxplots(df, columns_to_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Чистка выбросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Чистка выбросов и переименование\n",
    "def remove_outliers_IQR(df, for_clean):\n",
    "    df_cleaned = df.copy()\n",
    "    for column in for_clean:\n",
    "        original_count = df_cleaned.shape[0]\n",
    "        \n",
    "        Q1 = np.percentile(df_cleaned[column], 25)\n",
    "        Q3 = np.percentile(df_cleaned[column], 75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        df_cleaned = df_cleaned[(df_cleaned[column] >= lower_bound) & (df_cleaned[column] <= upper_bound)]\n",
    "        \n",
    "        # Количество убранных значений\n",
    "        cleaned_count = df_cleaned.shape[0]\n",
    "        removed_counts = original_count - cleaned_count\n",
    "        print(column , removed_counts)\n",
    "\n",
    "    return df_cleaned\n",
    "\n",
    "\n",
    "print(len(df))\n",
    "start_len = len(df)\n",
    "for_clean = ['words', 'likes_all']\n",
    "df = remove_outliers_IQR(df, for_clean)\n",
    "print(len(df))\n",
    "print(f'Cleaned: {start_len - len(df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Таблицы распределения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Указание стиля 'dark_background'\n",
    "num_zones = 10  # количество зон\n",
    "bins = np.linspace(df['words'].min(), df['words'].max(), num_zones + 1)\n",
    "\n",
    "# Добавление зоны в DataFrame\n",
    "df['zone'] = pd.cut(df['words'], bins, labels=False)\n",
    "\n",
    "# Средние значения в каждой зоне\n",
    "avg_points = df.groupby('zone').agg({'words': 'mean', 'likes_all': 'mean'}).reset_index()\n",
    "\n",
    "# Указание стиля 'dark_background'\n",
    "plt.style.use('default')\n",
    "\n",
    "# Построение scatter plot\n",
    "plt.figure(figsize=(30, 18))\n",
    "plt.scatter(\n",
    "    x=df['words'],\n",
    "    y=df['likes_all'],\n",
    "    color='#8A2BE2',  # насыщенный фиолетовый\n",
    "    edgecolor='black'  # чёрная обводка\n",
    ")\n",
    "\n",
    "# Добавление средних точек\n",
    "plt.scatter(\n",
    "    x=avg_points['words'],\n",
    "    y=avg_points['likes_all'],\n",
    "    color='#8A2BE2',\n",
    "    s=200,  # размер точек\n",
    "    label='Среднее значение для промежутка'\n",
    ")\n",
    "\n",
    "# Построение кусочной функции\n",
    "plt.plot(\n",
    "    avg_points['words'],\n",
    "    avg_points['likes_all'],\n",
    "    color='#5781ff',\n",
    "    linestyle='-',  # сплошная линия\n",
    "    linewidth=4,\n",
    "    label='Функция средних значений'\n",
    ")\n",
    "\n",
    "# Настройки графика\n",
    "plt.title('Распределение лайков и слов', fontsize=24, color='black')\n",
    "plt.xlabel('Слова', fontsize=20, color='black')\n",
    "plt.ylabel('Лайки', fontsize=20, color='black')\n",
    "\n",
    "# Легенда\n",
    "plt.legend(fontsize=18, loc='upper right', frameon=False)\n",
    "\n",
    "# Показ графика\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Таблица корреляций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('default')\n",
    "# Ваши данные\n",
    "columns_for_corr = ['emoji', 'tags', 'links', 'words', 'likes_all', 'likes_count (<21)', 'likes_count (>31)', 'theme', 'medium_photo_text', 'medium_photo_colorfulness', \n",
    "                    'difficult', 'date', 'emote']\n",
    "df_2 = df.loc[:, columns_for_corr].copy()\n",
    "\n",
    "# Кастомная цветовая палитра\n",
    "gradient_colors = [(1, 193/255, 136/255),  # FCC188\n",
    "                   (112/255, 66/255, 210/255),  # 7042D2\n",
    "                   (46/255, 0, 142/255)]  # 2E008E\n",
    "cmap = LinearSegmentedColormap.from_list(\"custom_gradient\", gradient_colors)\n",
    "\n",
    "# Создание тепловой карты корреляций\n",
    "plt.figure(figsize=(14, 12))\n",
    "heatmap = sns.heatmap(df_2.select_dtypes(include=['number']).corr(method='spearman'), \n",
    "                        annot=True, \n",
    "                        cmap=cmap,  # Кастомная палитра\n",
    "                        vmin=-1, \n",
    "                        vmax=1, \n",
    "                        fmt=\".2f\", \n",
    "                        linewidths=0.5,\n",
    "                        annot_kws={\"size\": 18},  # Увеличение шрифта аннотаций\n",
    "                        xticklabels=True,  # Метки по оси X\n",
    "                        yticklabels=True)  # Метки по оси Y\n",
    "\n",
    "# Заголовок и оси с увеличенным шрифтом\n",
    "heatmap.tick_params(axis='both', which='major', labelsize=14, colors='black')\n",
    "\n",
    "plt.show()\n",
    "#Основное\n",
    "#sber\n",
    "#emoji - likes <21 и сравнение likes >31\n",
    "#words - likes <21 и сравнение likes >31\n",
    "#match_tv\n",
    "#emoji - likes <21 и сравнение likes >31\n",
    "#words - likes <21 и сравнение likes >31\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, spearmanr\n",
    "import pandas as pd\n",
    "\n",
    "# Колонки для анализа\n",
    "columns_for_corr = ['emoji', 'tags', 'links', 'words', 'likes_all',\n",
    "                    'theme', 'medium_photo_text', 'medium_photo_colorfulness', \n",
    "                    'difficult', 'date', 'emote']\n",
    "\n",
    "# Выбираем только числовые колонки\n",
    "df_2 = df.loc[:, columns_for_corr].copy()\n",
    "\n",
    "# Целевая колонка\n",
    "target_column = 'likes_all'\n",
    "cnt = 0\n",
    "\n",
    "results = []\n",
    "for column in columns_for_corr:\n",
    "    if column != target_column:\n",
    "        r, p_value = spearmanr(df_2[column], df_2[target_column])\n",
    "        results.append({'Фактор': column, 'Коэффициент корреляции (r)': r, 'p-value': p_value})\n",
    "    \n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.round(7)\n",
    "\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тест Шапиро-Уилка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Генерация случайных данных (например, нормально распределенные)e=100)\n",
    "\n",
    "# Проведение теста Шапиро-Уилка\n",
    "\n",
    "# Вывод результатов\n",
    "\n",
    "# Колонки для анализа\n",
    "columns_for_corr = ['likes_all']\n",
    "\n",
    "# Выбираем только числовые колонки\n",
    "df_2 = df.loc[:, columns_for_corr].copy()\n",
    "\n",
    "# Целевая колонка\n",
    "# Расчет корреляции и p-value для каждой колонки\n",
    "results = []\n",
    "for column in columns_for_corr:\n",
    "        print(column)\n",
    "        statistic, p_value = stats.shapiro(df_2[column])\n",
    "        print(f\"Статистика теста: {statistic}\")\n",
    "        print(f\"P-значение: {p_value}\")\n",
    "\n",
    "        # Интерпретация результата\n",
    "        alpha = 0.05\n",
    "        if p_value > alpha:\n",
    "            print(\"Нет оснований отвергать нулевую гипотезу: данные нормально распределены.\")\n",
    "        else:\n",
    "            print(\"Отвергаем нулевую гипотезу: данные не нормально распределены.\")\n",
    "    \n",
    "# Сортируем по абсолютному значению коэффициента корреляции для удобства анализа\n",
    "#results_df = results_df.sort_values(by='Коэффициент корреляции (r)', key=abs, ascending=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Разбиение на равные группы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Функция для вычисления границ групп\n",
    "def calculate_bins(df):\n",
    "    bin_info = {}\n",
    "    for column in df.select_dtypes(include='number').columns:\n",
    "        unique_values = df[column].nunique()  # Проверка уникальных значений\n",
    "        if unique_values > 1:  # Если в колонке больше одного уникального значения\n",
    "            try:\n",
    "                bins = pd.qcut(df[column], q=3, duplicates='drop', retbins=True)[1]\n",
    "                bin_info[column] = bins\n",
    "            except ValueError:\n",
    "                bin_info[column] = \"Unable to split (insufficient variability)\"\n",
    "        else:\n",
    "            bin_info[column] = \"Single unique value\"\n",
    "    return bin_info\n",
    "\n",
    "# Пример данных\n",
    "\n",
    "# Вывод границ для каждой числовой колонки\n",
    "bins = calculate_bins(df)\n",
    "for col, bin_edges in bins.items():\n",
    "    print(f\"Column: {col}, Bins: {bin_edges}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Критерий Крускала-Уолеса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import kruskal\n",
    "\n",
    "# Загружаем данные из файла\n",
    "#data = df.copy()\n",
    "groups = ['emoji_group', 'links_group', 'tags_group', 'parts_group', 'time_group', 'words_group', 'colorfulness_group', 'difficult_group', 'medium_photo_text_bins']\n",
    "\n",
    "# Приведём данные к корректным типам, если потребуется\n",
    "data['emote'] = data['emote'].astype(int)  # Убедимся, что emote числовой\n",
    "\n",
    "# Разбивка переменной emoji на группы (Low, Medium, High)\n",
    "emoji_bins = [-1, 1, 2, float('inf')]\n",
    "emoji_labels = ['0', '0.5', '1']\n",
    "data['emoji_group'] = pd.cut(data['emoji'], bins=emoji_bins, labels=emoji_labels)\n",
    "\n",
    "links_bins = [-1, 3, 6, float('inf')]\n",
    "links_labels = ['0', '0.5', '1']\n",
    "data['links_group'] = pd.cut(data['links'], bins=links_bins, labels=links_labels)\n",
    "\n",
    "\n",
    "tags_bins = [-1, 1, 2, float('inf')]\n",
    "tags_labels = ['0', '0.5', '1']\n",
    "data['tags_group'] = pd.cut(data['tags'], bins=tags_bins, labels=tags_labels)\n",
    "\n",
    "parts_bins = [-1, 3, 4, float('inf')]\n",
    "parts_labels = ['0', '0.5', '1']\n",
    "data['parts_group'] = pd.cut(data['parts_cnt'], bins=parts_bins, labels=parts_labels)\n",
    "\n",
    "time_bins = [4, 10, 13, float('inf')]\n",
    "time_labels = ['0', '0.5', '1']\n",
    "data['time_group'] = pd.cut(data['hour'], bins=time_bins, labels=time_labels)\n",
    "\n",
    "words_bins = [-1, 58, 87, float('inf')]\n",
    "words_labels = ['0', '0.5', '1']\n",
    "data['words_group'] = pd.cut(data['words'], bins=words_bins, labels=words_labels)\n",
    "\n",
    "colorfulness_bins = [-1, 2, 7, float('inf')]\n",
    "colorfulness_labels = ['0', '0.5', '1']\n",
    "data['colorfulness_group'] = pd.cut(data['medium_photo_colorfulness'], bins=colorfulness_bins, labels=colorfulness_labels)\n",
    "\n",
    "difficult_bins = [1, 5, 7, float('inf')]\n",
    "difficult_labels = ['0', '0.5', '1']\n",
    "data['difficult_group'] = pd.cut(data['difficult'], bins=difficult_bins, labels=difficult_labels)\n",
    "\n",
    "text_bins = [0.0, 0.6, 0.9, 1.0]\n",
    "text_labels = ['0', '0.5', '1']  # Названия групп для удобства\n",
    "data['medium_photo_text_bins'] = pd.cut(df['medium_photo_text'], bins=text_bins, labels=text_labels)\n",
    "\n",
    "\n",
    "# Проведение теста Крускала-Уоллиса для likes_all по emoji_group\n",
    "\n",
    "\n",
    "kruskal_emoji = kruskal(\n",
    "    *[data.loc[data['emoji_group'] == group, 'likes_all'] for group in emoji_labels]\n",
    ")\n",
    "\n",
    "kruskal_emote = kruskal(\n",
    "    *[data.loc[data['emote'] == emote, 'likes_all'] for emote in [-1, 0, 1]]\n",
    ")\n",
    "\n",
    "kruskal_time = kruskal(\n",
    "    *[data.loc[data['time_group'] == hour, 'likes_all'] for hour in time_labels]\n",
    ")\n",
    "# Проведение теста Крускала-Уоллиса для likes_all по emoji_group\n",
    "\n",
    "kruskal_word = kruskal(\n",
    "    *[data.loc[data['words_group'] == group, 'likes_all'] for group in words_labels]\n",
    ")\n",
    "kruskal_colorfulness = kruskal(\n",
    "    *[data.loc[data['colorfulness_group'] == group, 'likes_all'] for group in colorfulness_labels]\n",
    ")\n",
    "kruskal_difficult = kruskal(\n",
    "    *[data.loc[data['difficult_group'] == group, 'likes_all'] for group in difficult_labels]\n",
    ")\n",
    "kruskal_text = kruskal(\n",
    "    *[data.loc[data['medium_photo_text_bins'] == group, 'likes_all'] for group in text_labels]\n",
    ")\n",
    "kruskal_parts = kruskal(\n",
    "    *[data.loc[data['parts_group'] == group, 'likes_all'] for group in parts_labels]\n",
    ")\n",
    "kruskal_links = kruskal(\n",
    "    *[data.loc[data['links_group'] == group, 'likes_all'] for group in links_labels]\n",
    ")\n",
    "\n",
    "kruskal_emoji, kruskal_emote, kruskal_time, kruskal_word, kruskal_colorfulness, kruskal_difficult, kruskal_text, kruskal_parts, kruskal_links\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Модель машинного обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "# Кластеризация\n",
    "data = df.copy()\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "data['cluster'] = kmeans.fit_predict(data[['emoji', 'words', 'medium_photo_text', 'medium_photo_colorfulness', 'hour', 'parts_cnt', 'emote', 'difficult']])\n",
    "\n",
    "# Делим данные на обучающую и тестовую выборки\n",
    "X = data[['emoji', 'words', 'medium_photo_text', 'hour', 'parts_cnt', 'emote', 'medium_photo_colorfulness', 'difficult', 'cluster']]\n",
    "y = data['likes_all']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Обучение модели\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Предсказания\n",
    "y_pred = model.predict(X_test)\n",
    "joblib.dump(model, 'random_forest_model.pkl')\n",
    "print(\"Модель сохранена в 'random_forest_model.pkl'\")\n",
    "# Оценка\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Среднеквадратическая ошибка: {mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Поиск доп. параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Пример данных (замените на свои)\n",
    "\n",
    "# Создание совокупных факторов\n",
    "data['few_words_and_no_emoji'] = (data['words'] < 60.0) & (data['emoji'] == 0) # Малое количество слов и эмодзи\n",
    "data['few_words_and_few_tags'] = (data['words'] < 60.0) & (data['emoji'] > 0) # Малое количество слов и эмодзи\n",
    "data['emoji_and_link'] = (data['emoji'] > 0) & (df['links'] > 0)  # Эмодзи и ссылка\n",
    "data['emoji_and_tags'] = (data['emoji'] > 0) & (df['links'] > 0)  # Эмодзи и ссылка\n",
    "data['low_difficulty_few_words'] = (data['difficult'] <= 5) & (data['words'] < 60.0)  # Сложный и длинный текст\n",
    "data['few_words_photo_text'] = (data['words'] < 60.0) & (data['medium_photo_text'] > 0)  # Малое количество слов и текст на фото\n",
    "data['many_words_big_difficult'] = (data['words'] > 60.0) & (data['difficult'] > 6)  # Эмодзи и низкая сложность текста\n",
    "\n",
    "# Преобразование булевых значений в числовые\n",
    "\n",
    "# Список факторов\n",
    "factors = ['many_words_big_difficult','few_words_photo_text', 'low_difficulty_few_words', 'emoji_and_tags', 'emoji_and_link', 'few_words_and_few_tags', 'few_words_and_no_emoji']\n",
    "\n",
    "# Корреляционный анализ\n",
    "correlation_results = {}\n",
    "for factor in factors:\n",
    "    corr, p_value = spearmanr(data[factor], data['likes_all'])\n",
    "    correlation_results[factor] = {'correlation': corr, 'p_value': p_value}\n",
    "\n",
    "# Вывод результатов корреляции\n",
    "for factor, result in correlation_results.items():\n",
    "    print(f\"Factor: {factor}\")\n",
    "    print(f\"  Spearman correlation: {result['correlation']:.7f}\")\n",
    "    print(f\"  P-value: {result['p_value']:.7f}\")\n",
    "    print()\n",
    "\n",
    "# Визуализация факторов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Итоговый XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "#data = data.drop(columns = ['Unnamed: 0.1', 'Unnamed: 0', 'items', 'count', 'list of users', 'text_id',  'likes_count (<21)', 'likes_count (>31)', 'cnt_comm', 'photo_ids', 'theme', 'tone', 'medium_photo_text_bins', 'date', 'likes', 'zone'])\n",
    "\n",
    "data['cluster'] = kmeans.fit_predict(data[['emoji', 'words', 'medium_photo_text', 'medium_photo_colorfulness', 'hour', 'parts_cnt', 'emote', 'difficult']])\n",
    "\n",
    "param_grid = {\n",
    "    \"max_depth\": [3,4, 5, 6, 7],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    \"subsample\": [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    \"colsample_bytree\": [0.4, 0.5, 0.6, 0.8, 1.0],\n",
    "    \"n_estimators\": [100, 200, 300, 400, 500]\n",
    "}\n",
    "\n",
    "X = data.drop(columns=['likes_all'])  # Убираем колонку с целевой переменной\n",
    "y = data['likes_all']  # Целевая переменная\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features = 1000)\n",
    "text_features = vectorizer.fit_transform(df['text_id']).toarray()\n",
    "import numpy as np\n",
    "X = np.hstack((X, text_features))\n",
    "xgb = XGBRegressor(objective='reg:squarederror')\n",
    "\n",
    "# Разделяем данные на тренировочные и тестовые выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(f\"Лучшие параметры: {grid_search.best_params_}\")\n",
    "print(f\"Лучший AUC: {-grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Сохранение обученной модели\n",
    "joblib.dump(grid_search.best_estimator_, \"best_xgb_model.pkl\")\n",
    "\n",
    "# Загрузка сохранённой модели\n",
    "loaded_model = joblib.load(\"best_xgb_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "df['hour'] = pd.to_datetime(df['date'], format='%H:%M:%S').dt.hour\n",
    "\n",
    "# One-Hot Encoding для 'emote'\n",
    "\n",
    "# Преобразование всех булевых значений в числовые\n",
    "\n",
    "# Выбор независимых переменных\n",
    "X = df[['emoji', 'tags', 'links', 'words', 'medium_photo_text', 'medium_photo_colorfulness', \n",
    "                    'difficult', 'hour']]\n",
    "\n",
    "# Целевая переменная\n",
    "y = df['likes_all']\n",
    "\n",
    "# Добавление константы для модели\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Разделение данных на обучающую и тестовую выборки\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Построение модели\n",
    "model = sm.OLS(y_train, X_train).fit()\n",
    "\n",
    "# Вывод результатов модели\n",
    "print(model.summary())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
